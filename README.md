# Sistema de Reconocimiento de Lenguaje de SeÃ±as en Tiempo Real

Un sistema interactivo de aprendizaje de lenguaje de seÃ±as que utiliza **MediaPipe**, **OpenCV**, **TensorFlow** y **Tkinter** para proporcionar reconocimiento de gestos en tiempo real con retroalimentaciÃ³n inmediata.

## ğŸ“‹ Tabla de Contenidos

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Requisitos del Sistema](#-requisitos-del-sistema)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [EjecuciÃ³n del Proyecto](#-ejecuciÃ³n-del-proyecto)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Uso de la AplicaciÃ³n](#-uso-de-la-aplicaciÃ³n)
- [Entrenamiento del Modelo](#-entrenamiento-del-modelo)

## âœ¨ CaracterÃ­sticas

- âœ… **DetecciÃ³n en tiempo real** de manos con MediaPipe
- âœ… **Reconocimiento de gestos** usando redes neuronales LSTM
- âœ… **Interfaz grÃ¡fica intuitiva** con Tkinter
- âœ… **Modo de prÃ¡ctica** para aprendizaje autodidacta
- âœ… **RetroalimentaciÃ³n inmediata** (Correcto/Incorrecto/No detectado)
- âœ… **5 gestos bÃ¡sicos** del lenguaje de seÃ±as americano:
  - ğŸ‘‹ Hola
  - ğŸ‘‹ AdiÃ³s  
  - âœŒï¸ Paz
  - ğŸ‘ SÃ­
  - â¤ï¸ Te quiero
- âœ… **Arquitectura modular** para fÃ¡cil expansiÃ³n
- âœ… **Captura de datos** para entrenamiento personalizado

## ğŸ–¥ï¸ Requisitos del Sistema

### Hardware MÃ­nimo
- **Procesador:** Intel i3 o AMD equivalente
- **RAM:** 4 GB mÃ­nimo (8 GB recomendado)
- **CÃ¡mara web:** HD (720p) o superior
- **Espacio en disco:** 2 GB libres

### Software
- **Sistema Operativo:** Windows 10/11, macOS 10.14+, o Ubuntu 18.04+
- **Python:** 3.8 - 3.10 âš ï¸ **IMPORTANTE: MediaPipe no es compatible con Python 3.11+**
- **CÃ¡mara web** funcional

## ğŸ“¦ InstalaciÃ³n

### Paso 1: Verificar la versiÃ³n de Python

```bash
python --version
# o
python3 --version
```

**âš ï¸ IMPORTANTE:** AsegÃºrate de tener Python 3.8, 3.9 o 3.10. Si tienes Python 3.11+, debes instalar una versiÃ³n compatible.

### Paso 2: Clonar el repositorio

```bash
git clone https://github.com/steevenGD/Hand_Lenguage.git
cd Hand_Lenguage
```

### Paso 3: Crear entorno virtual (RECOMENDADO)

#### En Windows:
```bash
python -m venv venv
venv\Scripts\activate
```

#### En macOS/Linux:
```bash
python3 -m venv venv
source venv/bin/activate
```

### Paso 4: Instalar dependencias

```bash
pip install --upgrade pip
pip install mediapipe opencv-python tensorflow numpy pillow scikit-learn
```

**Lista completa de dependencias:**
```
mediapipe>=0.10.0
opencv-python>=4.8.0
tensorflow>=2.13.0
numpy>=1.24.0
pillow>=10.0.0
scikit-learn>=1.3.0
```

## ğŸš€ EjecuciÃ³n del Proyecto

### OpciÃ³n 1: Ejecutar la aplicaciÃ³n principal

#### En PyCharm:
1. Abre `Hand_lenguage.py`
2. Haz clic en el botÃ³n **Run** â–¶ï¸ o presiona **Shift+F10**

#### En Visual Studio Code:
1. Abre `Hand_lenguage.py`
2. Presiona **F5** o ve a **Run â†’ Start Debugging**

#### Desde la terminal:
```bash
# AsegÃºrate de que el entorno virtual estÃ© activado
python Hand_lenguage.py
```

### OpciÃ³n 2: Capturar nuevos gestos (Opcional)

```bash
python Capture_Gesture.py
```

**Controles de captura:**
- **1-19:** Seleccionar gesto (1=Reposo, 2=Hola, 3=AdiÃ³s, 4=SÃ­, 5=Paz, 6=Te quiero)
- **s:** Iniciar captura de secuencia (60 frames) para modelos LSTM.
- **q:** Salir

### OpciÃ³n 3: Entrenar el modelo (Opcional)

```bash
python train_gesture_model.py
```

## ğŸ“ Estructura del Proyecto

```
Hand_Lenguage/
â”œâ”€â”€ Hand_lenguage.py          # AplicaciÃ³n principal con interfaz grÃ¡fica
â”œâ”€â”€ Capture_Gesture.py        # Sistema de captura de gestos
â”œâ”€â”€ train_gesture_model.py    # Entrenamiento del modelo LSTM
â”œâ”€â”€ modelo_gestos_lstm.keras  # Modelo entrenado
â”œâ”€â”€ labels_lstm.npy          # Etiquetas del modelo
â”œâ”€â”€ gestures_data.csv        # Datos de gestos en formato CSV
â”œâ”€â”€ imagenes_gestos/         # ImÃ¡genes de referencia
â”‚   â”œâ”€â”€ hola.png
â”‚   â”œâ”€â”€ adios.png
â”‚   â”œâ”€â”€ paz.png
â”‚   â”œâ”€â”€ si.png
â”‚   â””â”€â”€ te quiero.png
â”œâ”€â”€ sequences/               # Secuencias de entrenamiento
â”‚   â”œâ”€â”€ Hola_*.npy
â”‚   â”œâ”€â”€ Adios_*.npy
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md               # Este archivo
```

## ğŸ® Uso de la AplicaciÃ³n

### Pantalla Principal
1. **Practicar:** Modo de aprendizaje libre
2. **Jugar:** Modo de evaluaciÃ³n con puntuaciÃ³n

### Modo PrÃ¡ctica
1. Selecciona un gesto del menÃº desplegable
2. Observa la imagen de referencia
3. Realiza el gesto frente a la cÃ¡mara
4. Recibe retroalimentaciÃ³n inmediata:
   - ğŸŸ¢ **CORRECTO:** Gesto reconocido correctamente
   - ğŸ”´ **INCORRECTO:** Gesto no coincide
   - âšª **NO DETECTADO:** No se detectan manos

### Consejos para mejor reconocimiento:
- MantÃ©n buena iluminaciÃ³n
- Usa un fondo simple
- Coloca las manos claramente visibles
- MantÃ©n el gesto por 2-3 segundos
- AsegÃºrate de que la cÃ¡mara capture ambas manos si es necesario

## ğŸ§  Entrenamiento del Modelo

### Arquitectura del Modelo LSTM
- **Capa de entrada:** 126 caracterÃ­sticas (21 puntos Ã— 3 coordenadas Ã— 2 manos)
- **Secuencia temporal:** 60 frames (2 segundos a 30 FPS)
- **Capas LSTM:** 2 capas con 64 unidades cada una
- **Capa densa:** 64 neuronas con activaciÃ³n ReLU
- **Salida:** 5 clases (gestos) con activaciÃ³n softmax

### Proceso de entrenamiento:
1. **Captura de datos:** Usar `Capture_Gesture.py`
2. **Entrenamiento:** Ejecutar `train_gesture_model.py`
3. **ValidaciÃ³n:** El modelo se evalÃºa automÃ¡ticamente
4. **Guardado:** Se generan `modelo_gestos_lstm.keras` y `labels_lstm.npy`

